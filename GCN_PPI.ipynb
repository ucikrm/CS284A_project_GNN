{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52846dc8-ff77-49d0-bca2-1558de560e18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6069, Val: 0.4294, Test: 0.4304\n",
      "Epoch: 002, Loss: 0.5827, Val: 0.4022, Test: 0.4052\n",
      "Epoch: 003, Loss: 0.5705, Val: 0.4596, Test: 0.4669\n",
      "Epoch: 004, Loss: 0.5545, Val: 0.5031, Test: 0.5130\n",
      "Epoch: 005, Loss: 0.5462, Val: 0.5095, Test: 0.5184\n",
      "Epoch: 006, Loss: 0.5446, Val: 0.4912, Test: 0.4995\n",
      "Epoch: 007, Loss: 0.5390, Val: 0.4742, Test: 0.4815\n",
      "Epoch: 008, Loss: 0.5306, Val: 0.5135, Test: 0.5230\n",
      "Epoch: 009, Loss: 0.5246, Val: 0.4813, Test: 0.4901\n",
      "Epoch: 010, Loss: 0.5207, Val: 0.5024, Test: 0.5132\n",
      "Epoch: 011, Loss: 0.5159, Val: 0.5174, Test: 0.5301\n",
      "Epoch: 012, Loss: 0.5155, Val: 0.5312, Test: 0.5458\n",
      "Epoch: 013, Loss: 0.5115, Val: 0.5185, Test: 0.5306\n",
      "Epoch: 014, Loss: 0.5094, Val: 0.4798, Test: 0.4948\n",
      "Epoch: 015, Loss: 0.5036, Val: 0.5073, Test: 0.5211\n",
      "Epoch: 016, Loss: 0.5010, Val: 0.5099, Test: 0.5205\n",
      "Epoch: 017, Loss: 0.4974, Val: 0.5266, Test: 0.5376\n",
      "Epoch: 018, Loss: 0.4953, Val: 0.5183, Test: 0.5307\n",
      "Epoch: 019, Loss: 0.4924, Val: 0.5461, Test: 0.5583\n",
      "Epoch: 020, Loss: 0.4913, Val: 0.5502, Test: 0.5613\n",
      "Epoch: 021, Loss: 0.4925, Val: 0.5620, Test: 0.5726\n",
      "Epoch: 022, Loss: 0.4932, Val: 0.5141, Test: 0.5261\n",
      "Epoch: 023, Loss: 0.4885, Val: 0.5404, Test: 0.5541\n",
      "Epoch: 024, Loss: 0.4846, Val: 0.5527, Test: 0.5640\n",
      "Epoch: 025, Loss: 0.4828, Val: 0.5365, Test: 0.5485\n",
      "Epoch: 026, Loss: 0.4823, Val: 0.5545, Test: 0.5660\n",
      "Epoch: 027, Loss: 0.4804, Val: 0.5296, Test: 0.5418\n",
      "Epoch: 028, Loss: 0.4762, Val: 0.5467, Test: 0.5602\n",
      "Epoch: 029, Loss: 0.4732, Val: 0.5720, Test: 0.5843\n",
      "Epoch: 030, Loss: 0.4736, Val: 0.5164, Test: 0.5305\n",
      "Epoch: 031, Loss: 0.4730, Val: 0.5488, Test: 0.5596\n",
      "Epoch: 032, Loss: 0.4728, Val: 0.5551, Test: 0.5703\n",
      "Epoch: 033, Loss: 0.4706, Val: 0.5616, Test: 0.5730\n",
      "Epoch: 034, Loss: 0.4684, Val: 0.5527, Test: 0.5654\n",
      "Epoch: 035, Loss: 0.4669, Val: 0.5935, Test: 0.6025\n",
      "Epoch: 036, Loss: 0.4664, Val: 0.5634, Test: 0.5728\n",
      "Epoch: 037, Loss: 0.4640, Val: 0.5917, Test: 0.6009\n",
      "Epoch: 038, Loss: 0.4657, Val: 0.5483, Test: 0.5630\n",
      "Epoch: 039, Loss: 0.4636, Val: 0.5712, Test: 0.5810\n",
      "Epoch: 040, Loss: 0.4662, Val: 0.4937, Test: 0.5128\n",
      "Epoch: 041, Loss: 0.4634, Val: 0.5850, Test: 0.5960\n",
      "Epoch: 042, Loss: 0.4602, Val: 0.5706, Test: 0.5814\n",
      "Epoch: 043, Loss: 0.4586, Val: 0.5884, Test: 0.5981\n",
      "Epoch: 044, Loss: 0.4595, Val: 0.5780, Test: 0.5925\n",
      "Epoch: 045, Loss: 0.4600, Val: 0.5615, Test: 0.5690\n",
      "Epoch: 046, Loss: 0.4604, Val: 0.5563, Test: 0.5702\n",
      "Epoch: 047, Loss: 0.4597, Val: 0.5531, Test: 0.5623\n",
      "Epoch: 048, Loss: 0.4619, Val: 0.5694, Test: 0.5825\n",
      "Epoch: 049, Loss: 0.4548, Val: 0.5963, Test: 0.6057\n",
      "Epoch: 050, Loss: 0.4525, Val: 0.5998, Test: 0.6100\n",
      "Epoch: 051, Loss: 0.4520, Val: 0.5499, Test: 0.5604\n",
      "Epoch: 052, Loss: 0.4518, Val: 0.5586, Test: 0.5722\n",
      "Epoch: 053, Loss: 0.4545, Val: 0.5403, Test: 0.5549\n",
      "Epoch: 054, Loss: 0.4509, Val: 0.5765, Test: 0.5887\n",
      "Epoch: 055, Loss: 0.4501, Val: 0.5913, Test: 0.6034\n",
      "Epoch: 056, Loss: 0.4473, Val: 0.5872, Test: 0.5988\n",
      "Epoch: 057, Loss: 0.4466, Val: 0.5941, Test: 0.6054\n",
      "Epoch: 058, Loss: 0.4465, Val: 0.5722, Test: 0.5866\n",
      "Epoch: 059, Loss: 0.4465, Val: 0.5909, Test: 0.6037\n",
      "Epoch: 060, Loss: 0.4454, Val: 0.6012, Test: 0.6145\n",
      "Epoch: 061, Loss: 0.4447, Val: 0.5908, Test: 0.6056\n",
      "Epoch: 062, Loss: 0.4436, Val: 0.5871, Test: 0.6011\n",
      "Epoch: 063, Loss: 0.4443, Val: 0.5923, Test: 0.6063\n",
      "Epoch: 064, Loss: 0.4417, Val: 0.5931, Test: 0.6070\n",
      "Epoch: 065, Loss: 0.4453, Val: 0.5753, Test: 0.5932\n",
      "Epoch: 066, Loss: 0.4431, Val: 0.5765, Test: 0.5920\n",
      "Epoch: 067, Loss: 0.4431, Val: 0.5984, Test: 0.6134\n",
      "Epoch: 068, Loss: 0.4398, Val: 0.6090, Test: 0.6217\n",
      "Epoch: 069, Loss: 0.4393, Val: 0.6095, Test: 0.6214\n",
      "Epoch: 070, Loss: 0.4421, Val: 0.5927, Test: 0.6109\n",
      "Epoch: 071, Loss: 0.4453, Val: 0.5674, Test: 0.5782\n",
      "Epoch: 072, Loss: 0.4526, Val: 0.5601, Test: 0.5797\n",
      "Epoch: 073, Loss: 0.4455, Val: 0.6186, Test: 0.6312\n",
      "Epoch: 074, Loss: 0.4437, Val: 0.5957, Test: 0.6095\n",
      "Epoch: 075, Loss: 0.4403, Val: 0.6076, Test: 0.6222\n",
      "Epoch: 076, Loss: 0.4361, Val: 0.6030, Test: 0.6138\n",
      "Epoch: 077, Loss: 0.4335, Val: 0.6156, Test: 0.6297\n",
      "Epoch: 078, Loss: 0.4338, Val: 0.5837, Test: 0.6010\n",
      "Epoch: 079, Loss: 0.4336, Val: 0.6106, Test: 0.6248\n",
      "Epoch: 080, Loss: 0.4343, Val: 0.6035, Test: 0.6187\n",
      "Epoch: 081, Loss: 0.4378, Val: 0.5658, Test: 0.5813\n",
      "Epoch: 082, Loss: 0.4389, Val: 0.6199, Test: 0.6322\n",
      "Epoch: 083, Loss: 0.4376, Val: 0.6106, Test: 0.6236\n",
      "Epoch: 084, Loss: 0.4355, Val: 0.5762, Test: 0.5977\n",
      "Epoch: 085, Loss: 0.4353, Val: 0.6014, Test: 0.6173\n",
      "Epoch: 086, Loss: 0.4329, Val: 0.6053, Test: 0.6209\n",
      "Epoch: 087, Loss: 0.4306, Val: 0.6035, Test: 0.6194\n",
      "Epoch: 088, Loss: 0.4318, Val: 0.5792, Test: 0.5963\n",
      "Epoch: 089, Loss: 0.4286, Val: 0.6090, Test: 0.6223\n",
      "Epoch: 090, Loss: 0.4320, Val: 0.6292, Test: 0.6443\n",
      "Epoch: 091, Loss: 0.4353, Val: 0.6199, Test: 0.6309\n",
      "Epoch: 092, Loss: 0.4346, Val: 0.5943, Test: 0.6122\n",
      "Epoch: 093, Loss: 0.4364, Val: 0.6110, Test: 0.6239\n",
      "Epoch: 094, Loss: 0.4316, Val: 0.6034, Test: 0.6207\n",
      "Epoch: 095, Loss: 0.4285, Val: 0.6338, Test: 0.6447\n",
      "Epoch: 096, Loss: 0.4302, Val: 0.6240, Test: 0.6370\n",
      "Epoch: 097, Loss: 0.4345, Val: 0.6168, Test: 0.6326\n",
      "Epoch: 098, Loss: 0.4313, Val: 0.6027, Test: 0.6163\n",
      "Epoch: 099, Loss: 0.4302, Val: 0.6292, Test: 0.6420\n",
      "Epoch: 100, Loss: 0.4299, Val: 0.5612, Test: 0.5870\n",
      "Median time per epoch: 0.3646s\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv  # Importing GCN layer\n",
    "\n",
    "path = './ppi_data/'\n",
    "train_dataset = PPI(path, split='train')\n",
    "val_dataset = PPI(path, split='val')\n",
    "test_dataset = PPI(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "\n",
    "class GCNPPI(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(train_dataset.num_features, 256)\n",
    "        self.conv2 = GCNConv(256, 256)\n",
    "        self.conv3 = GCNConv(256, train_dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        ys.append(data.y)\n",
    "        out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        preds.append((out > 0).float().cpu())\n",
    "\n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0\n",
    "\n",
    "\n",
    "times = []\n",
    "for epoch in range(1, 101):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    val_f1 = test(val_loader)\n",
    "    test_f1 = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_f1:.4f}, '\n",
    "          f'Test: {test_f1:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513797fb-be6c-4965-b006-fb3c5f64b7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
