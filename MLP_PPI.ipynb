{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52846dc8-ff77-49d0-bca2-1558de560e18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.5750, Val: 0.4165, Test: 0.4175\n",
      "Epoch: 002, Loss: 0.5508, Val: 0.4184, Test: 0.4200\n",
      "Epoch: 003, Loss: 0.5388, Val: 0.4289, Test: 0.4327\n",
      "Epoch: 004, Loss: 0.5307, Val: 0.4634, Test: 0.4685\n",
      "Epoch: 005, Loss: 0.5197, Val: 0.4916, Test: 0.4987\n",
      "Epoch: 006, Loss: 0.5121, Val: 0.5195, Test: 0.5270\n",
      "Epoch: 007, Loss: 0.5100, Val: 0.5034, Test: 0.5106\n",
      "Epoch: 008, Loss: 0.5057, Val: 0.4946, Test: 0.5026\n",
      "Epoch: 009, Loss: 0.4986, Val: 0.5027, Test: 0.5108\n",
      "Epoch: 010, Loss: 0.4949, Val: 0.4954, Test: 0.5040\n",
      "Epoch: 011, Loss: 0.4916, Val: 0.5034, Test: 0.5125\n",
      "Epoch: 012, Loss: 0.4872, Val: 0.5221, Test: 0.5310\n",
      "Epoch: 013, Loss: 0.4854, Val: 0.5060, Test: 0.5161\n",
      "Epoch: 014, Loss: 0.4822, Val: 0.5021, Test: 0.5114\n",
      "Epoch: 015, Loss: 0.4815, Val: 0.5205, Test: 0.5297\n",
      "Epoch: 016, Loss: 0.4824, Val: 0.5397, Test: 0.5491\n",
      "Epoch: 017, Loss: 0.4768, Val: 0.5440, Test: 0.5535\n",
      "Epoch: 018, Loss: 0.4769, Val: 0.5309, Test: 0.5403\n",
      "Epoch: 019, Loss: 0.4747, Val: 0.5298, Test: 0.5392\n",
      "Epoch: 020, Loss: 0.4734, Val: 0.5209, Test: 0.5304\n",
      "Epoch: 021, Loss: 0.4719, Val: 0.5270, Test: 0.5366\n",
      "Epoch: 022, Loss: 0.4716, Val: 0.5359, Test: 0.5452\n",
      "Epoch: 023, Loss: 0.4701, Val: 0.5338, Test: 0.5431\n",
      "Epoch: 024, Loss: 0.4705, Val: 0.5351, Test: 0.5444\n",
      "Epoch: 025, Loss: 0.4707, Val: 0.5390, Test: 0.5485\n",
      "Epoch: 026, Loss: 0.4693, Val: 0.5368, Test: 0.5466\n",
      "Epoch: 027, Loss: 0.4684, Val: 0.5439, Test: 0.5533\n",
      "Epoch: 028, Loss: 0.4697, Val: 0.5412, Test: 0.5505\n",
      "Epoch: 029, Loss: 0.4681, Val: 0.5362, Test: 0.5458\n",
      "Epoch: 030, Loss: 0.4669, Val: 0.5326, Test: 0.5417\n",
      "Epoch: 031, Loss: 0.4674, Val: 0.5102, Test: 0.5192\n",
      "Epoch: 032, Loss: 0.4674, Val: 0.5323, Test: 0.5412\n",
      "Epoch: 033, Loss: 0.4669, Val: 0.5318, Test: 0.5409\n",
      "Epoch: 034, Loss: 0.4675, Val: 0.5404, Test: 0.5492\n",
      "Epoch: 035, Loss: 0.4668, Val: 0.5306, Test: 0.5397\n",
      "Epoch: 036, Loss: 0.4669, Val: 0.5346, Test: 0.5435\n",
      "Epoch: 037, Loss: 0.4670, Val: 0.5345, Test: 0.5435\n",
      "Epoch: 038, Loss: 0.4657, Val: 0.5286, Test: 0.5380\n",
      "Epoch: 039, Loss: 0.4658, Val: 0.5239, Test: 0.5329\n",
      "Epoch: 040, Loss: 0.4653, Val: 0.5476, Test: 0.5563\n",
      "Epoch: 041, Loss: 0.4656, Val: 0.5351, Test: 0.5444\n",
      "Epoch: 042, Loss: 0.4650, Val: 0.5368, Test: 0.5455\n",
      "Epoch: 043, Loss: 0.4652, Val: 0.5333, Test: 0.5423\n",
      "Epoch: 044, Loss: 0.4649, Val: 0.5356, Test: 0.5447\n",
      "Epoch: 045, Loss: 0.4646, Val: 0.5455, Test: 0.5544\n",
      "Epoch: 046, Loss: 0.4644, Val: 0.5375, Test: 0.5462\n",
      "Epoch: 047, Loss: 0.4638, Val: 0.5300, Test: 0.5387\n",
      "Epoch: 048, Loss: 0.4643, Val: 0.5251, Test: 0.5339\n",
      "Epoch: 049, Loss: 0.4638, Val: 0.5434, Test: 0.5523\n",
      "Epoch: 050, Loss: 0.4651, Val: 0.5260, Test: 0.5350\n",
      "Epoch: 051, Loss: 0.4639, Val: 0.5451, Test: 0.5541\n",
      "Epoch: 052, Loss: 0.4635, Val: 0.5317, Test: 0.5402\n",
      "Epoch: 053, Loss: 0.4637, Val: 0.5405, Test: 0.5492\n",
      "Epoch: 054, Loss: 0.4639, Val: 0.5408, Test: 0.5495\n",
      "Epoch: 055, Loss: 0.4630, Val: 0.5434, Test: 0.5519\n",
      "Epoch: 056, Loss: 0.4649, Val: 0.5372, Test: 0.5455\n",
      "Epoch: 057, Loss: 0.4636, Val: 0.5345, Test: 0.5437\n",
      "Epoch: 058, Loss: 0.4626, Val: 0.5466, Test: 0.5553\n",
      "Epoch: 059, Loss: 0.4631, Val: 0.5409, Test: 0.5498\n",
      "Epoch: 060, Loss: 0.4630, Val: 0.5392, Test: 0.5478\n",
      "Epoch: 061, Loss: 0.4634, Val: 0.5438, Test: 0.5525\n",
      "Epoch: 062, Loss: 0.4638, Val: 0.5377, Test: 0.5465\n",
      "Epoch: 063, Loss: 0.4627, Val: 0.5430, Test: 0.5516\n",
      "Epoch: 064, Loss: 0.4629, Val: 0.5307, Test: 0.5395\n",
      "Epoch: 065, Loss: 0.4626, Val: 0.5488, Test: 0.5570\n",
      "Epoch: 066, Loss: 0.4638, Val: 0.5359, Test: 0.5446\n",
      "Epoch: 067, Loss: 0.4639, Val: 0.5420, Test: 0.5506\n",
      "Epoch: 068, Loss: 0.4624, Val: 0.5512, Test: 0.5595\n",
      "Epoch: 069, Loss: 0.4635, Val: 0.5333, Test: 0.5416\n",
      "Epoch: 070, Loss: 0.4624, Val: 0.5387, Test: 0.5475\n",
      "Epoch: 071, Loss: 0.4626, Val: 0.5311, Test: 0.5398\n",
      "Epoch: 072, Loss: 0.4631, Val: 0.5387, Test: 0.5475\n",
      "Epoch: 073, Loss: 0.4624, Val: 0.5376, Test: 0.5462\n",
      "Epoch: 074, Loss: 0.4622, Val: 0.5349, Test: 0.5435\n",
      "Epoch: 075, Loss: 0.4620, Val: 0.5327, Test: 0.5416\n",
      "Epoch: 076, Loss: 0.4628, Val: 0.5431, Test: 0.5515\n",
      "Epoch: 077, Loss: 0.4626, Val: 0.5424, Test: 0.5509\n",
      "Epoch: 078, Loss: 0.4639, Val: 0.5442, Test: 0.5527\n",
      "Epoch: 079, Loss: 0.4628, Val: 0.5307, Test: 0.5387\n",
      "Epoch: 080, Loss: 0.4625, Val: 0.5376, Test: 0.5458\n",
      "Epoch: 081, Loss: 0.4620, Val: 0.5341, Test: 0.5430\n",
      "Epoch: 082, Loss: 0.4619, Val: 0.5371, Test: 0.5448\n",
      "Epoch: 083, Loss: 0.4611, Val: 0.5483, Test: 0.5568\n",
      "Epoch: 084, Loss: 0.4626, Val: 0.5311, Test: 0.5391\n",
      "Epoch: 085, Loss: 0.4625, Val: 0.5391, Test: 0.5472\n",
      "Epoch: 086, Loss: 0.4615, Val: 0.5490, Test: 0.5577\n",
      "Epoch: 087, Loss: 0.4622, Val: 0.5448, Test: 0.5530\n",
      "Epoch: 088, Loss: 0.4628, Val: 0.5446, Test: 0.5529\n",
      "Epoch: 089, Loss: 0.4614, Val: 0.5429, Test: 0.5516\n",
      "Epoch: 090, Loss: 0.4612, Val: 0.5344, Test: 0.5426\n",
      "Epoch: 091, Loss: 0.4611, Val: 0.5429, Test: 0.5514\n",
      "Epoch: 092, Loss: 0.4618, Val: 0.5404, Test: 0.5488\n",
      "Epoch: 093, Loss: 0.4613, Val: 0.5428, Test: 0.5517\n",
      "Epoch: 094, Loss: 0.4617, Val: 0.5470, Test: 0.5558\n",
      "Epoch: 095, Loss: 0.4621, Val: 0.5319, Test: 0.5407\n",
      "Epoch: 096, Loss: 0.4615, Val: 0.5439, Test: 0.5523\n",
      "Epoch: 097, Loss: 0.4612, Val: 0.5426, Test: 0.5512\n",
      "Epoch: 098, Loss: 0.4612, Val: 0.5436, Test: 0.5522\n",
      "Epoch: 099, Loss: 0.4627, Val: 0.5418, Test: 0.5506\n",
      "Epoch: 100, Loss: 0.4614, Val: 0.5368, Test: 0.5453\n",
      "Median time per epoch: 0.2820s\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "path = './ppi_data/'\n",
    "train_dataset = PPI(path, split='train')\n",
    "val_dataset = PPI(path, split='val')\n",
    "test_dataset = PPI(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "class MLPPPI(torch.nn.Module):\n",
    "    def __init__(self, input_features, hidden_size, output_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = Linear(input_features, hidden_size)\n",
    "        self.fc2 = Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = Linear(hidden_size, output_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Adjust the model initialization to use the new MLP class\n",
    "input_features = train_dataset.num_features\n",
    "hidden_size = 256\n",
    "output_classes = train_dataset.num_classes\n",
    "\n",
    "\n",
    "\n",
    "# Rest of the code remains the same\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MLPPPI(input_features, hidden_size, output_classes).to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x), data.y)  # Only pass data.x\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        ys.append(data.y)\n",
    "        out = model(data.x.to(device))  # Only pass data.x\n",
    "        preds.append((out > 0).float().cpu())\n",
    "\n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0\n",
    "\n",
    "\n",
    "\n",
    "times = []\n",
    "for epoch in range(1, 101):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    val_f1 = test(val_loader)\n",
    "    test_f1 = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_f1:.4f}, '\n",
    "          f'Test: {test_f1:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513797fb-be6c-4965-b006-fb3c5f64b7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
